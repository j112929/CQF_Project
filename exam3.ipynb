{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1_M8sPmduyb"
      },
      "source": [
        "#Section 1 Classifier and Hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4leAXKLvtvk"
      },
      "source": [
        "\n",
        "###1.(a)\n",
        "###Original function of the logistic classfier:\n",
        "$$y=σ(f(x))=σ(ω^Tx)=\\frac{1}{1+e^{-ω^Tx}},  $$\n",
        "### where ω is weights;\n",
        "\n",
        "\n",
        "###The total probability of a set of data:\n",
        "$$ P_{total}=P(y_1|x_1)P(y_2|x_2)P(y_3|x_3)....P(y_N|x_N) \\\\\n",
        "  =Π_{n=1}^Np^{y_n}(1-p)^{1-y_n}  $$\n",
        "\n",
        "\n",
        "###Loss function of the logistic classfier:\n",
        "$$ F_{loss}(ω)=ln(P_{total})=ln(Π_{n=1}^Np^{y_n}(1-p)^{1-y_n}) \\\\\n",
        "=\\sum_{n=1}^Nln(p^{y_n}(1-p)^{1-y_n}) \\\\\n",
        "=\\sum_{n=1}^N(y_nln(p)+(1-{y_n})ln(1-p))\\\\\n",
        "$$\n",
        "### where\n",
        "$$  \\mathtt{p=\\frac{1}{1+e^{-ω^Tx}}}\n",
        "$$\n",
        "### MLE(Maximum Likelihood Estimation):\n",
        "$$ ω^* = argmaxF(ω)= -argminF(ω) $$\n",
        "\n",
        "\\(b)\n",
        "### Ordinary Least Squares (OLS)\n",
        "$$L_{OLS}(\\hatβ̂)=\\sum_{i=1}^n(y_i-x_i^j\\hatβ̂)^2=||y-X\\hatβ̂||^2 $$\n",
        "### where\n",
        "$$\\hatβ̂_{OLS}= (X'X)^{-1}(X'Y)$$\n",
        "### The penalty expressions of The Ridge regressions\n",
        "$$ L_{ridge}(\\hatβ̂ ) = \\sum_{i=1}^n(y_i-x_i^j\\hatβ̂)^2 +  λ\\sum_{j=1}^m\\hatβ̂_j^2 \\\\\n",
        "= ||y-X\\hatβ̂||^2+λ||\\hatβ̂||^2\n",
        " $$\n",
        "#### Solving this for β̂  gives the the ridge regression estimates $$\\hatβ̂_{ ridge}=(X′X+λI)^{−1}(X′Y) ,$$\n",
        "#### where I denotes the identity matrix.\n",
        "#### The λ parameter is the regularization penalty. We will talk about how to choose it in the next sections of this tutorial, but for now notice that:\n",
        "$$ As λ→0,\\hatβ̂_{ridge}→\\hatβ̂_{OLS}; \\\\\n",
        "As λ→∞,\\hatβ̂_{ridge}→0.$$\n",
        "####So, setting λ to 0 is the same as using the OLS, while the larger its value, the stronger is the coefficients' size penalized.\n",
        "\n",
        "#### The only difference in ridge and lasso loss functions is in the penalty terms. Under lasso, the loss is defined as:\n",
        "$$L_{lasso}(\\hatβ̂)= \\sum_{i=1}^n(y_i-x_i^j\\hatβ̂)^2 + λ\\sum_{j=1}^m|\\hatβ̂_j|$$\n",
        "\n",
        "\\(c)\n",
        "####Mean Squared Error:\n",
        "$$ L_{MSE}(y-\\hat{y})= \\frac{1}{n}\\sum_{i=1}^n(y_i-\\hat{y_i})^2 $$\n",
        "####Mean Squared Loss:\n",
        "$$ L_{MSE}(y-\\hat{y})= \\sum_{i=1}^n(y_i-\\hat{y_i})^2 $$\n",
        "\n",
        "####RSS(Residual sum of squares) :\n",
        "$$ RSS= \\sum_{i=1}^n(y_i-\\hat{y_i})^2 $$\n",
        "\n",
        "\\\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geyfAYtp2j7B"
      },
      "source": [
        "####2.(a)\n",
        "\n",
        "Ridge|Lasso\n",
        "--|--\n",
        "Not very robust|Robust\n",
        "Stable solution| Unstable solution\n",
        "Always one solution|Possibly multiple solutions\n",
        "\n",
        "#### (b)For the Ridge : From its equation, you can see that as λ becomes larger, the variance decreases, and the bias increases or unchanges.\n",
        "#### For the Lasso: This additional term penalizes the model for having coefficients that do not explain a sufficient amount of variance in the data.\n",
        "####As λ increses,shrinkage occurs so that variables that are at zero can be thrown away."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAVYNTZGxsxD"
      },
      "source": [
        "#Section 2 Model Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCD6bpu0x7KD"
      },
      "source": [
        "1. Return to preprocessing and fit one model using 2 types of scaling on features data. Produce histograms of changed data columns. Report on changes to coefficients.\n",
        "(a) min-max scaling or robust quantiles-based scaling;\n",
        "(b) scaling to uniform [0, 1] range."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5x6w_sZaMuX"
      },
      "source": [
        "\n",
        "```\n",
        "返回预处理，使用特征数据的两种类型的缩放拟合一个模型。生成已更改的数据列的直方图。报告对系数的变化。(a)最小最大比例或稳健量化基于文件的缩放；(b)缩放到统一的[0,1]范围\n",
        "```\n",
        "Answer:\n",
        "\n",
        "Group A：min-max scaling :all feature values are in the range [0, 1];\n",
        "\n",
        "Group A：robust quantiles-based scaling:the resulting range of the transformed feature values is larger than for the previous scalers;\n",
        "\n",
        "Group B：min-max scaling :all feature values are in the range [0, 1];\n",
        "\n",
        "Group B：robust quantiles-based scaling:the resulting range of the transformed feature values is larger than for the previous scalers;\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5grVfRpPWO_"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIoPtSGqzWA1"
      },
      "source": [
        "2. Choose a restricted set of features, use penalisation and other (below), and produce a model that\n",
        "makes predictions with reduced variance (MSE). Accuracy score is not estimator variance.\n",
        "\n",
        "(a) formally explain your approach to feature selection or elimination. For example, VIF for elimi\u0002nation of interdependent (colinear) features, or selection according to a score from the F-test.\n",
        "\n",
        "(b) if prediction task allows (Group A, restricted model) graphically plot a logistic sigmoid for each\n",
        "feature. If prediction is not binomial (Group B), sigmoid only possible between two classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpCqFmSzawsn"
      },
      "source": [
        "```\n",
        "#选择一组受限特征，使用惩罚和其他（下文），并生成一个以减少方差进行预测的模型(MSE)。精度分数不是估计量的方差。\n",
        "#(a)正式的解释您的功能选择或消除的方法。例如，VIF用于消除相互依赖的（共线性）特征，或根据f测试中的分数进行选择。\n",
        "#(b)如果是预测任务（A组，受限模型）,允许图形化地绘制每个特征的逻辑sigmoid函数。如果预测不是二项式（B组），则sigmoid只能在两类之间进行绘制。\n",
        "```\n",
        "Answer:\n",
        "\n",
        "(a)Group A: To predict Google stock, select all remaining columns except 'Volumn' and perform logistic regression training on column 'Adj Close' with penalty L2. The reason is that by observing the previous Sigmoid graph, it is found that the column 'Volumn' and column 'Adj Close' are in linear relationship rather than logical relationship, so it is excluded.\n",
        "\n",
        "Group B:The data is from A Five-Factor Asset Pricing Model Fama & French, and the column 'MKT-RF' is analyzed.\n",
        "Using logistic regression, penalty is L1, Solver is liblinear.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBQn6Mm8zhWK"
      },
      "source": [
        "3. For the winning restricted model produce a suitable set of Evaluation metrics, such as area under ROC curve\n",
        "(each class) and confusion matrix. Give expressions for precision/recall."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vna0sCFPbsU"
      },
      "source": [
        "Answer:\n",
        "\n",
        "\n",
        "####Receiver Operating Characteristic curve is a curve with false positive rate (FP_rate) and false negative rate (TP_rate) as axes. The area under the ROC curve is called AUC;\n",
        "\n",
        "\n",
        "\n",
        "precision:\n",
        "$$ P_{precision}=\\frac{TP}{TP+FP}$$\n",
        "\n",
        "recall:\n",
        "$$ R_{recall} = \\frac{TP}{TP+FN} = \\frac{TP}{P} = R_{sensitive}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcES6DR10Znh"
      },
      "source": [
        "#Section 3 Mathematical bases of supervised learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKmwfOvg0eH_"
      },
      "source": [
        "1.(a)can there exist an estimator with the smaller MSE than minimal least squares?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jqykg-KSyNAj"
      },
      "source": [
        "Answer:Yes, for example,Median-unbiased estimators:\n",
        "$$β̂^* = E[{β̂}^\\sim |T] $$\n",
        "where $$β̂^\\sim$$ is any estimator, T is a sufficient statistic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXyJm3TnyK8i"
      },
      "source": [
        "(b) for a prediction, does the MSE measure an irreducible error or model error?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpP--eI1yRdD"
      },
      "source": [
        "Answer:Yes, For a prediction,In order to ensure more accurate prediction results, MSE is used to measure an irreducible error."
      ]
    }
  ]
}